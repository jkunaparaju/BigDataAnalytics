{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Spark is cluster computing framework designed to handle large datasets. Spark makes unique in this type computing because its design to cover wide range of work loads  in one place. It reduces burden of maintaining different tools. \
Spark can run on had clusters and access hadoop data. It provides API in python , java ,scala and SQL. Spark provides closely integrated components at its core provides many benefits like when core engine adds optimization its Machinel earning libraries  also benefits from core. \
\
Spark Stack :  Spark Core : Provides Task Schdduling , memory management , fault recovery , interacting with storage\'85 Maininly it provdies api to define RDD (Resilient distributes datasets) which will represent collection of Items distributed across nodes to manipulate parallel.\
\
Spark Sql : SaprkPackage to work with structured data. Both SQL and hive sql are used to query data from deign formats like Hive table , Parquet , JSON formats. \
\
Spark Steaming : Its used to process live streams of data. Ex: Log files web servers. Most of the API simlar to Core RDD api making it easy to understand to move between applications. \
\
MLIB and GraphX. Mlib provides MachineLearning algorithms and GraphX is for manipulating graphs.\
\
Spark can run many cluster managers like Hadoop YARN , Apache Mesos. \
\
There two kind of tasks spark will provide \
\
Data Science Taks: \
Spark shell is easy to interact for data scientists  }